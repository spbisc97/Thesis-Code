{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71161059-8567-496c-b651-b210f29835ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "from stable_baselines3.common.noise import (\n",
    "    NormalActionNoise,\n",
    "    OrnsteinUhlenbeckActionNoise,\n",
    ")\n",
    "from safegym.envs import Satellite_SE2\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.time_limit import TimeLimit\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "import safegym\n",
    "#!tensorboard --logdir=/app/simonerotondi/thesis/savings/ & /app/simonerotondi/libs/ngrook/ngrok http --basic-auth='simone:rotondi97' 6006 --authtoken '--' & fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c489731-9591-4968-867b-162521201786",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Satellite-SE2-v0\"\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "Algo = A2C\n",
    "Algo_name = \"A2C\"\n",
    "# ENT = 0.01\n",
    "use_last_model = False\n",
    "\n",
    "if use_last_model:\n",
    "    date: str = input(\"Insert date: \")\n",
    "    last_model: int = int(input(\"Insert model number: \"))\n",
    "else:\n",
    "    date: str = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n",
    "    last_model: int = 0\n",
    "\n",
    "print({\"date\": date, \"last_model\": last_model})\n",
    "time.sleep(5)\n",
    "\n",
    "top_dir = \"savings/\"\n",
    "models_dir = top_dir + f\"{env_name}/{Algo_name}/{date}/models/\"\n",
    "logdir = top_dir + f\"{env_name}/{Algo_name}/{date}/logs/\"\n",
    "imgs_dir = top_dir + f\"{env_name}/{Algo_name}/{date}/imgs/\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "os.makedirs(imgs_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb88a6a-a23f-4ce6-b5d8-c99f1de63d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_reward_file(imgs_dir: str,**kwargs):\n",
    "    import inspect\n",
    "    from pprint import pprint\n",
    "\n",
    "    rewfile = open(f\"{imgs_dir}/Reward.md\", \"w\")\n",
    "    print(\"```{python}\", file=rewfile)\n",
    "    print(inspect.getsource(Satellite_SE2._reward_function), file=rewfile)\n",
    "    print(\"```\", file=rewfile)\n",
    "    print(\"```{python}\", file=rewfile)\n",
    "    pprint(kwargs,stream=rewfile)\n",
    "    print(\"```\", file=rewfile)\n",
    "    rewfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc045550-e1c3-4db9-ad72-5da9bfb10da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    model, env, model_name=\"A2C\", model_num=0, model_timesteps=0, **kargs\n",
    "):\n",
    "    term = False\n",
    "    obs, info = env.reset()\n",
    "    while not term:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "        if term or trunc:\n",
    "            X = np.array(env.render(), dtype=np.uint8)\n",
    "            plt.imsave(\n",
    "                f\"{imgs_dir}/{model_name}_{model_num}_{model_timesteps:.1e}.png\",\n",
    "                X,\n",
    "            )\n",
    "            term = False\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cb8e2-3ca1-44c0-bc56-22ca7eb18c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0: np.float32 = np.float32(10)  # [m]\n",
    "# STARTING_STATE=\n",
    "radius: np.float32 = y0  # [m],\n",
    "speed_dev: np.float32 = np.float32(0)  # [m/s],\n",
    "theta: np.float32 = np.float32(0)  # [rad],\n",
    "theta_dot: np.float32 = np.float32(0)  # [rad/s],\n",
    "phi: np.float32 = np.float32(0)  # [rad]\n",
    "phi_dot: np.float32 = np.float32(0)  # [rad/s]\n",
    "STARTING_STATE = np.array(\n",
    "    [radius, speed_dev, theta, theta_dot, phi, phi_dot], dtype=np.float32\n",
    ")\n",
    "\n",
    "radius_noise: np.float32 = np.float32(5)\n",
    "speed_noise_multiplier: np.float32 = np.float32(0.01)\n",
    "theta_noise: np.float32 = np.float32(np.pi * 2)\n",
    "theta_dot_noise: np.float32 = np.float32(1e-3)\n",
    "phi_noise: np.float32 = np.float32(0)\n",
    "phi_dot_noise: np.float32 = np.float32(0)\n",
    "\n",
    "STARTING_NOISE = np.array(\n",
    "    [\n",
    "        radius_noise,\n",
    "        speed_noise_multiplier,\n",
    "        theta_noise,\n",
    "        theta_dot_noise,\n",
    "        phi_noise,\n",
    "        phi_dot_noise,\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "initial_integraton_steps = np.array([0, 400], dtype=np.int32)\n",
    "# REWARD_WEIGHTS = distance_decrease,-distance,-action,-speed,-angle_speed\n",
    "REWARD_WEIGHTS = np.array([20, 0.8, 0.5, 1, 30], dtype=np.float32)\n",
    "\n",
    "\n",
    "env_params={\n",
    "    \"starting_state\":STARTING_STATE,\n",
    "    \"starting_noise\":STARTING_NOISE,\n",
    "    \"initial_integration_steps\":initial_integraton_steps,\n",
    "    \"underactuated\":True,\n",
    "     \"step\":0.1,\n",
    "    \"reward_weights\":REWARD_WEIGHTS,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f6dc6-5881-47c4-b864-852b20d85f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_maker(render_mode=None):\n",
    "    env = gym.make(\n",
    "        env_name,\n",
    "        render_mode=render_mode,\n",
    "        **env_params\n",
    "    )\n",
    "\n",
    "    env = TimeLimit(env, max_episode_steps=60_000)\n",
    "    env = Monitor(env)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47485059-b563-47eb-9ccc-b4d3b2403655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make_vec_env(env_maker, n_envs=2)\n",
    "\n",
    "# env = gym.make(env_name)\n",
    "n_actions = 2\n",
    "\n",
    "params = {\n",
    "    \"mean\": np.zeros(n_actions),\n",
    "    \"sigma\": np.array([1e-3, 1e-5], dtype=np.float32),  # np.ones(n_actions\n",
    "    \"dtype\": np.float32,\n",
    "}\n",
    "O_params = {\n",
    "    \"theta\": 0.2,\n",
    "    \"dt\": 1e-2,\n",
    "    \"initial_noise\": None,\n",
    "}\n",
    "#action_noise = OrnsteinUhlenbeckActionNoise(**params, **O_params)\n",
    "action_noise = NormalActionNoise(**params)\n",
    "\n",
    "params_episode = {\n",
    "    \"env\": env_maker(render_mode=\"rgb_array_graph\"),\n",
    "    \"model_name\": Algo_name,\n",
    "}\n",
    "params_common_algo = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "    \"verbose\": 1,\n",
    "    \"tensorboard_log\": logdir,\n",
    "    \"stats_window_size\": 30,\n",
    "}\n",
    "\n",
    "params_algo = {\n",
    "    \"learning_rate\":0.0007, \n",
    "    \"n_steps\":20, \n",
    "    \"gamma\":0.999, \n",
    "    \"gae_lambda\":1.0, \n",
    "    \"ent_coef\":0.01, \n",
    "    \"vf_coef\":0.5, \n",
    "    \"max_grad_norm\":0.5, \n",
    "    \"rms_prop_eps\":1e-05, \n",
    "    \"use_rms_prop\":True, \n",
    "    \"use_sde\":False, \n",
    "    \"sde_sample_freq\":-1, \n",
    "    \"normalize_advantage\":False,\n",
    "    \"policy_kwargs\": dict(net_arch=[512, 512, 512]),\n",
    "    \"stats_window_size\":30,\n",
    "    \n",
    "}\n",
    "\n",
    "TIMESTEPS = 200_000\n",
    "params_learn = {\n",
    "    \"total_timesteps\": TIMESTEPS,\n",
    "    \"reset_num_timesteps\": False,\n",
    "    \"log_interval\": 2,\n",
    "    \"progress_bar\": False,\n",
    "    #\"callback\": EpisodeEndCallback(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edeb1d-3de6-429c-907c-20ec19bc89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if last_model > 0:\n",
    "    model = Algo.load(\n",
    "        f\"{models_dir}/{Algo_name}_{last_model}\",\n",
    "        **params_algo,\n",
    "        _init_setup_model=False,\n",
    "    )\n",
    "else:\n",
    "    # input(\"Press Enter to delete logs and models\")\n",
    "    # send2trash.send2trash(f\"{logdir}/\")\n",
    "    # send2trash.send2trash(f\"{models_dir}/\")\n",
    "    model = Algo(\n",
    "        **params_algo,\n",
    "        _init_setup_model=True,\n",
    "        \n",
    "        \n",
    "    )\n",
    "    fill_reward_file(imgs_dir,params_algo=params_algo,params_learn=params_learn,params=params,O_params=O_params,env_params=env_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bab18-766a-4183-97c4-5b0451ecc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(\n",
    "    model,\n",
    "    **params_episode,\n",
    "    model_num=last_model,\n",
    "    model_timesteps=model.num_timesteps,\n",
    "    kargs=(),\n",
    ")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model, env, n_eval_episodes=1, deterministic=True\n",
    ")\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20e34d-aada-4413-85d2-52c78c618756",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "for i in range(last_model + 1, last_model + epochs + 1):\n",
    "    model.learn(\n",
    "        **params_learn,\n",
    "        tb_log_name=f\"run_{i}\",\n",
    "    )\n",
    "    model.save(f\"{models_dir}/{Algo_name}_{i}\")\n",
    "    last_model = i\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, env, n_eval_episodes=2, deterministic=True\n",
    "    )\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    run_episode(\n",
    "        model,\n",
    "        **params_episode,\n",
    "        model_num=last_model,\n",
    "        model_timesteps=model.num_timesteps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597e726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
